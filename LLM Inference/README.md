# AdaDecode âœ¨

**Faster Large Language Model (LLM) Decoding using Adaptive Layer Parallelism**
---

## ğŸš€ What is AdaDecode?

**AdaDecode** is a decoding technique for LLMs that makes text generation **faster** without changing the model or using extra helper models. It works by **predicting some tokens early** (before all layers are done) and starting the next token's processing right away.

> "If the model is already confident about the next word, let's not wait. Just go!"

---

## âœ¨ Key Features

* âœ… Faster decoding (up to **1.73x** speedup)
* âœ… No need for extra models (unlike speculative decoding)
* âœ… Keeps output the **same** as normal decoding (verified)
* âœ… Easy to plug into existing transformer models

---

## âš–ï¸ How It Works (Simple Explanation)

In normal decoding:

* The model thinks **very deeply** (all layers) before writing each word.
* It goes step by step: token â†’ token â†’ token.

In AdaDecode:

1. The model **guesses early** if it's confident (e.g., at layer 20 instead of 50).
2. It **starts the next word immediately**.
3. Meanwhile, it **finishes the skipped layers in the background**.
4. It checks: was the early guess correct?

   * If YES âœ…: move on!
   * If NO âŒ: redo properly (rollback).

### ğŸ”¹ Example:

Generating: `The cat is sleeping.`

Normal decoding:

```
Think 50 layers â†’ 'The'
Think 50 layers â†’ 'cat'
Think 50 layers â†’ 'is' ...
```

AdaDecode:

```
Think 20 layers â†’ (confident) â†’ 'cat' â†’ start next
Finish layers 21-50 for 'cat' in background
Verify later
```

---

## âš ï¸ Downsides

* Might rollback if early guess was wrong (some overhead)
* More complex than standard decoding
* Works best when tokens are predictable

---

## ğŸ“† Citation

If you use AdaDecode, please cite:

```
@article{wei2025adadecode,
  title={AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism},
  author={Wei, Zhepei and Chen, Wei-Lin and Zhu, Xinyu and Meng, Yu},
  journal={arXiv preprint arXiv:2506.03700},
  year={2025}
}
```

---

## ğŸ”— Links

* [ğŸ“„ Paper on arXiv](https://arxiv.org/abs/2506.03700v1)
* [ğŸ“ Official Code (GitHub)](https://github.com/weizhepei/AdaDecode)

---

Made with â¤ï¸ by the community. Contributions welcome!

